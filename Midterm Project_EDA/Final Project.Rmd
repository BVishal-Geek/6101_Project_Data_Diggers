---
title: "Untitled"
output: html_document
date: "2023-11-30"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}
data1<- read.csv("/Users/qibinhuang/Downloads/Asteroid_Hazard_Classification.csv")
data <- data.frame(data1)
```

#

```{r}
library(caret)
library(ROSE)
library(DMwR)

set.seed(123)  # Set a random seed for reproducibility
splitIndex <- createDataPartition(data1$Hazardous, p = 0.80, list = FALSE)
train_data <- data1[splitIndex, ]
test_data <- data1[-splitIndex, ]

table(train_data$Hazardous)  # Check the class distribution in the training set
str(data1$Hazardous)
train_data$Hazardous <- train_data$Hazardous + 1
table(train_data$Hazardous)
str(data1$Hazardous)
```




```{r}
## Oversampling the minority class in the training data
train_data_over <- ovun.sample(Hazardous ~ ., data = train_data, method = "over", N = 2 * nrow(train_data))$data

summary(train_data_over)


```

```{r}
## Undersampling
library(caret)
# Convert 'Hazardous' to a factor
train_data$Hazardous <- as.factor(train_data$Hazardous)
down_sampled <- downSample(x = train_data[, !names(train_data) %in% "Hazardous"], 
                           y = train_data$Hazardous)

table(down_sampled$Class)

```

```{r}
library(caret)
set.seed(123) # for reproducibility

# Assuming 'data' is your dataset and 'Hazardous' is the target variable
splitIndex <- createDataPartition(data$Hazardous, p = 0.80, list = FALSE)
train_data <- data[splitIndex, ]
test_data <- data[-splitIndex, ]

#1. Logistic Regression
# Check the levels of each factor variable
sapply(train_data, function(x) if(is.factor(x)) levels(x))



```








```{r}
###Question 3 ： What will be the variables, such as Estimated Diameter, Orbital Characteristics, RelativVelocity, Diameter, that exert the most significant influence on the accuracy of models ?

library(tidyverse)
library(caret)
library(randomForest)





set.seed(123)  # Setting a seed for reproducibility
train_index <- sample(1:nrow(data), 0.8 * nrow(data))  # 80% for training
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
##random forest
train_data$Hazardous <- as.factor(train_data$Hazardous)
test_data$Hazardous <- as.factor(test_data$Hazardous)

#random forest model
rf_model <- randomForest(Hazardous ~ ., data = train_data, na.action = na.omit)
print(rf_model)

#This model demonstrates high accuracy. It accurately classified 2491 non-hazardous objects (in here is TRUE negatives) with only 3 misclassifications (in here is FALSE positives), and correctly identified 445 hazardous objects (TRUE positives) with 14 misclassifications (FALSE negatives). The model's class error rates are low, at 0.12% for non-hazardous and 3.05% for hazardous classes. So The "Minimum.Orbit.Intersection" variable is highlighted as the most significant predictor in this classification task.


# Generate predictions
predictions <- predict(rf_model, test_data)
# Generate a confusion matrix
confusionMatrix <- table(Predicted = predictions, Actual = test_data$Hazardous)
print(confusionMatrix)
#The confusion matrix indicates the model's high accuracy, with only 3 incorrect predictions and very low rates of false positives and negatives. This proving the model's effectiveness in classifying objects as hazardous or non-hazardous.


#evaulate

importance(rf_model)
varImpPlot(rf_model)
#According to output and graph  the 'Minimum.Orbit.Intersection' is by far the most significant variable in the Random Forest model, as indicated by its high MeanDecreaseGini value. The Gini value more high means that the variable plays a more significant role.



##compare with predict value


library(caret)

# Create a confusion matrix
conf_matrix <- confusionMatrix(comparison$Predicted, comparison$Actual)

# Output the confusion matrix and summary statistics
print(conf_matrix)

```



```{r}
#Question1
# Do logistical regression
## to make sure response variable is factor
train_data$Hazardous <- as.factor(train_data$Hazardous)

# Convert all other variables to numeric
numeric_vars <- setdiff(names(train_data), "Hazardous")
train_data[numeric_vars] <- lapply(train_data[numeric_vars], function(x) as.numeric(as.character(x)))

# Check for any conversion errors
sapply(train_data, function(x) sum(is.na(x)))


# Delete NA row
train_data_clean <- na.omit(train_data)


train_data_clean$Hazardous <- as.factor(train_data_clean$Hazardous)

# check
sapply(train_data_clean, class)

# Delete these variable: Close.Approach.Date, Orbiting.Body, Orbit.Determination.Date and Equinox。
train_data_clean$Close.Approach.Date <- NULL
train_data_clean$Orbiting.Body <- NULL
train_data_clean$Orbit.Determination.Date <- NULL
train_data_clean$Equinox <- NULL
train_data_clean$Name <- NULL


str(train_data_clean)

logistic_model <- glm(Hazardous ~ ., family = binomial(link = "logit"), data = train_data_clean)

summary(logistic_model)
##something wrong, so we need to deal with multicollinearity
# Keep one variable and remain another
train_data_reduced <- train_data_clean %>%
  select(-Est.Dia.in.KM.max., -Est.Dia.in.Feet.min., -Est.Dia.in.Feet.max.)
# Do 'Est.Dia.in.KM.min.' log transfer also add 1 to avoid 0 for log
train_data_reduced$Est.Dia.in.KM.min.log <- log(train_data_reduced$Est.Dia.in.KM.min. + 1)

# Try model again
logistic_model_revised <- glm(Hazardous ~ . - Est.Dia.in.KM.min., 
                              family = binomial(link = "logit"), 
                              data = train_data_reduced)

summary(logistic_model_revised)

##The added logarithmic conversion variable Est.Dia.in.KM.min.log significantly affects the target variable Hazardous, which has a significant coefficient and a high z value. This shows that the transformation helps to improve the explanatory power of the model.
## we still need do some change

# check the variable distribution
table(train_data_reduced$Hazardous)

# Apply resampling technique

library(ROSE)
train_data_balanced <- ovun.sample(Hazardous ~ ., data = train_data_reduced, method = "both", N = 2000)$data

# Refit the model
logistic_model_balanced <- glm(Hazardous ~ ., family = binomial, data = train_data_balanced)

# Predict
test_data$Est.Dia.in.KM.min.log <- log(test_data$Est.Dia.in.KM.min. + 1)

predictions <- predict(logistic_model_balanced, test_data, type = "response")
predicted_class <- ifelse(predictions > 0.5, "TRUE", "FALSE")
predicted_class <- as.factor(predicted_class)

# calculate
conf_matrix <- table(Predicted = predicted_class, Actual = test_data$Hazardous)
print(conf_matrix)
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
print(paste("Accuracy:", accuracy))


##The methods of oversampling and undersampling help the model learn and recognize the features of each class better by adjusting the number of samples between classes. In my logistic regression model, I resampled the ROSE package to balance the class distribution of the target variable Hazardous. This is mainly achieved by a combination of oversampling a minority class (i.e. hazardous objects) and undersampling a majority class (i.e. non-hazardous objects). By this method model can learn the features of these samples more fully during training.
##The effect of this approach can be seen in the performance of the final model. Prior to the use of resampling techniques, models might be overly biased toward predicting the majority of classes, resulting in inadequate recognition of minority classes. But after applying resampling, the accuracy of the model improved significantly to about 95.26%, indicating that the model performed better at balancing the two categories.
##In general, techniques such as oversampling, undersampling, and SMOTE help improve the model's predictive power for all classes by balancing class proportions, thereby enhancing the overall performance of the model.


library(dplyr)
library(lubridate)

# clean some outlier
remove_outliers <- function(data, column) {
  Q1 <- quantile(data[[column]], 0.25, na.rm = TRUE)
  Q3 <- quantile(data[[column]], 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  data[data[[column]] >= lower_bound & data[[column]] <= upper_bound, ]
}

# Log transfer
reduce_skewness <- function(data, column) {
  data[[column]] <- log(data[[column]] + 1)
  data
}

num_vars <- c('Absolute.Magnitude', 'Est.Dia.in.KM.min.', 'Est.Dia.in.KM.max.', 'Est.Dia.in.Feet.min.', 
              'Est.Dia.in.Feet.max.', 'Epoch.Date.Close.Approach', 'Relative.Velocity.km.per.sec', 
              'Miss.Dist..Astronomical.', 'Miss.Dist..lunar.', 'Miss.Dist..kilometers.', 
              'Orbit.ID', 'Orbit.Uncertainity', 'Minimum.Orbit.Intersection', 'Jupiter.Tisserand.Invariant', 
              'Epoch.Osculation', 'Eccentricity', 'Semi.Major.Axis', 'Inclination', 'Asc.Node.Longitude', 
              'Orbital.Period', 'Perihelion.Distance', 'Perihelion.Arg', 'Aphelion.Dist', 'Perihelion.Time', 
              'Mean.Anomaly', 'Mean.Motion')

for (var in num_vars) {
  your_data <- remove_outliers(data, var)
  your_data <- reduce_skewness(data, var)
}


date_vars <- c('Close.Approach.Date', 'Orbit.Determination.Date')

for (var in date_vars) {
  your_data[[var]] <- ymd_hms(your_data[[var]])
}

str(your_data)


library(dplyr)
library(lubridate)

# transfer these variables to factor
your_data$Orbiting.Body <- as.factor(your_data$Orbiting.Body)
your_data$Equinox <- as.factor(your_data$Equinox)

# delete the date column
your_data <- select(your_data, -c(Close.Approach.Date, Orbit.Determination.Date, Year, Month, Day, Hour, Minute))

```





```{r}
library(ggplot2)
library(corrplot)

# 1. boxplot
ggplot(data1, aes(x = as.factor(Hazardous), y = Est.Dia.in.KM.min.)) +
  geom_boxplot() +
  xlab("Hazardous") +
  ylab("Estimated Diameter (KM)")

# 2. scatterplot
ggplot(data1, aes(x = Est.Dia.in.KM.min., y = Relative.Velocity.km.per.sec)) +
  geom_point(aes(color = as.factor(Hazardous))) +
  xlab("Estimated Diameter (KM)") +
  ylab("Relative Velocity (km/s)")

# 3. correlation matrix
numeric_vars <- data1[, sapply(data1, is.numeric)]
corr_matrix <- cor(numeric_vars, use = "complete.obs")
corrplot(corr_matrix, method = "circle")




```{r}
library(ggplot2)
library(rpart)
library(rpart.plot)

# Visualization

# Estimated Diameter vs Hazardous
ggplot(data1, aes(x = Est.Dia.in.KM.min., fill = as.factor(Hazardous))) +
  geom_histogram(bins = 30) +
  labs(title = "Estimated Diameter vs Hazardous", x = "Estimated Diameter (KM)", y = "Count")
#This histogram displays the vast majority of asteroids, both hazardous and non-hazardous, have smaller diameters, with counts dropping off as diameter increases. There is an overlap in the diameter ranges of hazardous and non-hazardous asteroids, suggesting that diameter alone may not be a definitive predictor of hazard.

# Orbital Inclination vs Hazardous
ggplot(data1, aes(x = Inclination, fill = as.factor(Hazardous))) +
  geom_histogram(bins = 30) +
  labs(title = "Orbital Inclination vs Hazardous", x = "Inclination (Degrees)", y = "Count")

#This histogram illustrates the relationship between the orbital inclination of asteroids and their classification as hazardous. There is a noticeable concentration of non-hazardous asteroids at lower inclinations, while hazardous asteroids are more evenly distributed across a range of inclinations.


# Relative.Velocity.km.per.sec
ggplot(data1, aes(x = Relative.Velocity.km.per.sec, fill = as.factor(Hazardous))) +
  geom_histogram(bins = 30) +
  labs(title = "Relative Velocity vs Hazardous", x = "Relative Velocity (km/sec)", y = "Count")
#This histogram shows the distribution of asteroids' relative velocity, divided by hazard classification. Both hazardous and non-hazardous asteroids exhibit a wide range of velocities, but there is a significant number of hazardous asteroids at higher velocities.


# build decision tree

# using Est.Dia.in.KM.min., Relative.Velocity.km.per.sec, Inclination as predict variable
decision_tree_model <- rpart(Hazardous ~ Est.Dia.in.KM.min. + Relative.Velocity.km.per.sec + Inclination, 
                             data = data1, 
                             method = "class")

rpart.plot(decision_tree_model)

#The decision tree model shows that smaller estimated diameters (Est.Dia.in.KM.min.) are predominantly classified as non-hazardous. As the diameter increases, the inclination becomes a factor, with asteroids having an inclination greater than or equal to 6.5 degrees being more frequently classified as hazardous. Additionally, asteroids with relative velocity less than 5.9 km/sec are also more likely to be classified as hazardous.


```


```{r}
library(ggplot2)

numeric_vars <- data1[, sapply(data1, is.numeric)]

# standard data
numeric_vars_scaled <- scale(numeric_vars)

pca_result <- prcomp(numeric_vars_scaled)

summary(pca_result)

# Scatterplot
pca_data <- as.data.frame(pca_result$x)
ggplot(pca_data, aes(x = PC1, y = PC2, color = data1$Hazardous)) +
  geom_point() +
  labs(title = "PCA - First two principal components", x = "PC1", y = "PC2")


#The PCA scatter plot analysis of asteroid data shows significant overlap between hazardous and non-hazardous asteroids within the first two principal components (PC1 and PC2), indicating that these components might not be adequate for a distinct classification of asteroids by hazard potential. Although PC1 and PC2 capture a considerable amount of data variability, the spread along the PC1 axis, especially for non-hazardous asteroids, suggests that there are other important dimensions of variability not encompassed by these two components.


#The PCA component importance analysis reveals that the first two principal components, PC1 and PC2, account for 25.84% and 19.81% of the data's variance respectively, totaling 45.65%. This suggests the need for additional components to capture the majority of the data's variance. The cumulative variance increases more slowly after the tenth component, indicating the necessity of a large number of components to explain a significant portion of the total variance. To reach a typical threshold of 70-90% cumulative variance for a comprehensive analysis, incorporating more than the first two principal components is essential.

#Conclusion:

#The PCA analysis shows that the first two principal components, though capturing significant variance, are insufficient for effectively distinguishing hazardous from non-hazardous asteroids. A more accurate classification may require additional components or alternative dimensionality reduction methods. Incorporating domain-specific knowledge could also improve the model's interpretability and classification accuracy.









```

```
Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
